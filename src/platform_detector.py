# -*- coding: utf-8 -*-
"""platform_detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/119_dzp3hiyZ609Sy5odUNughEWyIwD_P

# **Platform detector**
"""

# from google.colab import drive

# drive.mount('/content/drive', force_remount=True)

# All nececcasary imports
import os
import torch
import torchvision
from torchvision.io import read_image, ImageReadMode
from torchvision.ops.boxes import masks_to_boxes
from torchvision import tv_tensors
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor
import torchvision.transforms.functional as F
from torch.utils.data import Dataset
from torch.utils.data.dataloader import DataLoader
from torchvision.io import read_image
from torchvision.transforms import v2 as T
import random
from tqdm import tqdm
from torchvision.utils import draw_bounding_boxes
# Fix random seed
random.seed(0)
torch.manual_seed(0)
ROOT_FOLDER = 'borders-recognition-data-2.0'

assert ROOT_FOLDER is not None, "[!] Enter the foldername."

device = torch.device("mps") if torch.backends.mps.is_available() else 'cpu'

# Create class for platform dataset, implementing nececcasary __init__, __getitem__ and __len__ methods

class PlatformDataset(Dataset):
    def __init__(self, path_to_imgs, path_to_masks, transforms = None):
        self.path_to_imgs = os.path.join(ROOT_FOLDER, path_to_imgs)
        self.path_to_masks = os.path.join(ROOT_FOLDER, path_to_masks)
        self.image_ids = list(sorted(os.listdir(os.path.join(ROOT_FOLDER, path_to_imgs))))
        self.mask_ids = list(sorted(os.listdir(os.path.join(ROOT_FOLDER, path_to_masks))))
        self.transforms = transforms

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]
        mask_id = self.mask_ids[idx]

        img = read_image(os.path.join(self.path_to_imgs, img_id), mode=ImageReadMode.RGB)
        mask = read_image(os.path.join(self.path_to_masks, mask_id), mode=ImageReadMode.GRAY)
        # Number of unique objects in the mask
        obj_ids = torch.unique(mask)
        # First object is background so it's omitted
        obj_ids = obj_ids[1:]
        num_objs = len(obj_ids)

        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)

        n = num_objs
        # We have only one class
        labels = torch.ones((n,), dtype=torch.int64)

        # Create bounding boxes from masks
        boxes = masks_to_boxes(masks)

        image_id = self.image_ids[idx]
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])

        img = tv_tensors.Image(img)

        # target must contain masks, boxes and labels keys

        target = {}
        target["boxes"] = tv_tensors.BoundingBoxes(boxes, format="XYXY", canvas_size=F.get_image_size(img), dtype=torch.float32)
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["masks"] = tv_tensors.Mask(masks)

        if self.transforms is not None:
            img, target = self.transforms(img, target)

        return img, target

    def __len__(self):
        return len(self.image_ids)

def get_model_instance_segmentation(num_classes):
    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights="DEFAULT")

    in_features = model.roi_heads.box_predictor.cls_score.in_features

    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    model.roi_heads.mask_predictor = MaskRCNNPredictor(
        in_features_mask,
        hidden_layer,
        num_classes
    )

    return model

def get_transform(train):
    transforms = []
    transforms.append(T.Resize(size=(224, 224), antialias=True))
    if train:
        transforms.append(T.RandomHorizontalFlip(0.5))
    transforms.append(T.ToDtype(torch.float, scale=True))
    transforms.append(T.ToPureTensor())
    transforms.append(T.Normalize(
            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
        ))
    return T.Compose(transforms)

def data_collate_fn(batch):
    return tuple(zip(*batch))

def platform_detector():
    print(device)
    # Draw bounding boxes on validation set to make sure everything works correctly

    train_set = PlatformDataset('dataset', 'masks-platform', get_transform(train=True)) # Training set (90% of dataset)
    val_set = PlatformDataset('validation-set', 'validation-masks-platform', get_transform(train=False)) # Validation set (10% of dataset)

    model = get_model_instance_segmentation(num_classes=2)
    model.to(device)

    loaders = {'training': DataLoader(train_set, num_workers=2, batch_size=4, shuffle=True, collate_fn = data_collate_fn),
           'validation':DataLoader(val_set, num_workers=2, shuffle=False, collate_fn = data_collate_fn)}
    boxes_output = {}

    predicted_boxes = {}

    out = {}

    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(
        params,
        lr=0.005,
        momentum=0.9,
        weight_decay=0.0005
    )
    num_epochs = 25
    for epoch in range(num_epochs):
        print('Current epoch: ' + str(epoch + 1))
        for mode in ['training', 'validation']:
            with torch.set_grad_enabled(mode == 'training'):
                model.train() if mode == 'training' else model.eval()
                for inputs, targets in tqdm(loaders[mode]):
                        inputs = list(img.to(device) for img in inputs)
                        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]
                        if mode == 'validation': # Validation stage
                            outputs = model(inputs)
                            if epoch == num_epochs-1:
                                predicted_boxes[targets[0]['image_id']] = outputs[0]['boxes'] # Bounding boxes predictions
                                boxes_output[targets[0]['image_id']] =  targets[0]['boxes'] # Actual bounding boxes
                                out[targets[0]['image_id']] = outputs[0]
                        if mode == 'training': # Training stage
                            outputs = model(inputs, targets)
                            loss = sum(loss for loss in outputs.values())
                            optimizer.zero_grad() # Nulling the gradient
                            loss.backward() # Calculating the gradient
                            optimizer.step() # Weights update
    print("That's it!")

    image_ids = list(sorted(os.listdir(os.path.join(ROOT_FOLDER, 'validation-set'))))
    masks_ids = list(sorted(os.listdir(os.path.join(ROOT_FOLDER, 'validation-masks-platform'))))
    colors1 = ['green']
    colors2 = ['red']
    for i in range(len(image_ids)):
        img_id = image_ids[i]
        masks_id = masks_ids[i]
        if len(boxes_output[img_id]) == 0:
            continue
        validation_set = os.path.join(ROOT_FOLDER, 'validation-set')
        validation_masks_platform = os.path.join(ROOT_FOLDER, 'validation-masks-platform')
        img = read_image(os.path.join(validation_set, img_id), mode=ImageReadMode.RGB)
        img = torchvision.transforms.Resize(size=(224, 224), antialias=True)(img)
        mask = read_image(os.path.join(validation_masks_platform, masks_id), mode=ImageReadMode.GRAY)
        obj_ids = torch.unique(mask)
        obj_ids = obj_ids[1:]
        img = img.to(device)
        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)
        masks = torchvision.transforms.Resize(size=(224, 224), antialias=True)(masks)
        new_img = draw_bounding_boxes(img, predicted_boxes[img_id], colors=colors1, fill=True, width=1)
        new_img = draw_bounding_boxes(new_img, boxes_output[img_id], colors=colors2, fill=True, width=1)
        new_img = torchvision.transforms.ToPILImage()(new_img)
        new_img.show()
        new_img.save('predicted_' + str(img_id))

if __name__ == '__main__':
    platform_detector()